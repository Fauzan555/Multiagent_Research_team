{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9aec0c",
   "metadata": {},
   "source": [
    "# We are going to built a multi-agent research system using LangGraph and Google's Gemini API, featuring specialized agents—Researcher, Analyst, Writer, and Supervisor. Each agent will handle a specific stage of the research process, working together to automate data collection, analysis, and report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bee5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langgraph langchain-google-genai langchain-community langchain-core python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba50d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "422b11d5",
   "metadata": {},
   "source": [
    "# Begin by installing the necessary libraries, including LangGraph and LangChain’s Google Gemini integration. Then, we import the essential modules and set up our environment by securely entering the Google API key using the getpass module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da192b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"paste your api key here\"\n",
    "\n",
    "import os\n",
    "from typing import Annotated, List, Tuple, Union\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import functools\n",
    "\n",
    "import getpass\n",
    "GOOGLE_API_KEY = getpass.getpass(\"paste your api key here \")\n",
    "os.environ[\"paste your api key here\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76237355",
   "metadata": {},
   "source": [
    "# We are going to define two TypedDict classes to maintain structured state and responses shared across all agents in the LangGraph. AgentState tracks messages, workflow status, topic, and collected findings, while AgentResponse standardizes each agent’s output. We also create a helper function to start the Gemini LLM with a specified model and temperature, ensuring consistent behavior across all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e76846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State shared between all agents in the graph\"\"\"\n",
    "    messages: Annotated[list, operator.add]\n",
    "    next: str\n",
    "    current_agent: str\n",
    "    research_topic: str\n",
    "    findings: dict\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "class AgentResponse(TypedDict):\n",
    "    \"\"\"Standard response format for all agents\"\"\"\n",
    "    content: str\n",
    "    next_agent: str\n",
    "    findings: dict\n",
    "\n",
    "\n",
    "def create_llm(temperature: float = 0.1, model: str = \"gemini-1.5-flash\") -> ChatGoogleGenerativeAI:\n",
    "    \"\"\"Create a configured Gemini LLM instance\"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b79c4d",
   "metadata": {},
   "source": [
    "# Created our first specialized agent, the Research Specialist AI. This agent is prompted to deeply analyze a given topic, extract key areas of interest, and suggest directions for further exploration. Using a ChatPromptTemplate, we define its behavior and connect it with our Gemini LLM. The research_agent function executes this logic, updates the shared state with findings and messages, and passes control to the next agent in line, the Analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8502b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_agent(llm: ChatGoogleGenerativeAI) -> callable:\n",
    "    \"\"\"Creates a research specialist agent for initial data gathering\"\"\"\n",
    "   \n",
    "    research_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a Research Specialist AI. Your role is to:\n",
    "        1. Analyze the research topic thoroughly\n",
    "        2. Identify key areas that need investigation\n",
    "        3. Provide initial research findings and insights\n",
    "        4. Suggest specific angles for deeper analysis\n",
    "       \n",
    "        Focus on providing comprehensive, accurate information and clear research directions.\n",
    "        Always structure your response with clear sections and bullet points.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"Research Topic: {research_topic}\")\n",
    "    ])\n",
    "   \n",
    "    research_chain = research_prompt | llm\n",
    "   \n",
    "    def research_agent(state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute research analysis\"\"\"\n",
    "        try:\n",
    "            response = research_chain.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"research_topic\": state[\"research_topic\"]\n",
    "            })\n",
    "           \n",
    "            findings = {\n",
    "                \"research_overview\": response.content,\n",
    "                \"key_areas\": [\"area1\", \"area2\", \"area3\"],\n",
    "                \"initial_insights\": response.content[:500] + \"...\"\n",
    "            }\n",
    "           \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "                \"next\": \"analyst\",\n",
    "                \"current_agent\": \"researcher\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": {**state.get(\"findings\", {}), \"research\": findings},\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "           \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Research agent error: {str(e)}\"\n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)],\n",
    "                \"next\": \"analyst\",\n",
    "                \"current_agent\": \"researcher\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "   \n",
    "    return research_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c304f0c",
   "metadata": {},
   "source": [
    "# Now defined the Data Analyst AI, which dives deeper into the research findings generated by the previous agent. This agent identifies key patterns, trends, and metrics, offering actionable insights backed by evidence. Using a tailored system prompt and the Gemini LLM, the analyst_agent function enriches the state with structured analysis, preparing the groundwork for the report writer to synthesize everything into a final document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2adb45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analyst_agent(llm: ChatGoogleGenerativeAI) -> callable:\n",
    "    \"\"\"Creates a data analyst agent for deep analysis\"\"\"\n",
    "   \n",
    "    analyst_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a Data Analyst AI. Your role is to:\n",
    "        1. Analyze data and information provided by the research team\n",
    "        2. Identify patterns, trends, and correlations\n",
    "        3. Provide statistical insights and data-driven conclusions\n",
    "        4. Suggest actionable recommendations based on analysis\n",
    "       \n",
    "        Focus on quantitative analysis, data interpretation, and evidence-based insights.\n",
    "        Use clear metrics and concrete examples in your analysis.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"Analyze the research findings for: {research_topic}\")\n",
    "    ])\n",
    "   \n",
    "    analyst_chain = analyst_prompt | llm\n",
    "   \n",
    "    def analyst_agent(state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute data analysis\"\"\"\n",
    "        try:\n",
    "            response = analyst_chain.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"research_topic\": state[\"research_topic\"]\n",
    "            })\n",
    "           \n",
    "            analysis_findings = {\n",
    "                \"analysis_summary\": response.content,\n",
    "                \"key_metrics\": [\"metric1\", \"metric2\", \"metric3\"],\n",
    "                \"recommendations\": response.content.split(\"recommendations:\")[-1] if \"recommendations:\" in response.content.lower() else \"No specific recommendations found\"\n",
    "            }\n",
    "           \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "                \"next\": \"writer\",\n",
    "                \"current_agent\": \"analyst\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": {**state.get(\"findings\", {}), \"analysis\": analysis_findings},\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "           \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Analyst agent error: {str(e)}\"\n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)],\n",
    "                \"next\": \"writer\",\n",
    "                \"current_agent\": \"analyst\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "   \n",
    "    return analyst_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be056654",
   "metadata": {},
   "source": [
    "# Now created the Report Writer AI, which is responsible for transforming the collected research and analysis into a polished, structured document. This agent synthesizes all previous insights into a clear, professional report with an executive summary, detailed findings, and conclusions. By invoking the Gemini model with a structured prompt, the writer agent updates the final report in the shared state and hands control over to the Supervisor agent for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ed34ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer_agent(llm: ChatGoogleGenerativeAI) -> callable:\n",
    "    \"\"\"Creates a report writer agent for final documentation\"\"\"\n",
    "   \n",
    "    writer_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a Report Writer AI. Your role is to:\n",
    "        1. Synthesize all research and analysis into a comprehensive report\n",
    "        2. Create clear, professional documentation\n",
    "        3. Ensure proper structure with executive summary, findings, and conclusions\n",
    "        4. Make complex information accessible to various audiences\n",
    "       \n",
    "        Focus on clarity, completeness, and professional presentation.\n",
    "        Include specific examples and actionable insights.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"Create a comprehensive report for: {research_topic}\")\n",
    "    ])\n",
    "   \n",
    "    writer_chain = writer_prompt | llm\n",
    "   \n",
    "    def writer_agent(state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute report writing\"\"\"\n",
    "        try:\n",
    "            response = writer_chain.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"research_topic\": state[\"research_topic\"]\n",
    "            })\n",
    "           \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "                \"next\": \"supervisor\",\n",
    "                \"current_agent\": \"writer\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": response.content\n",
    "            }\n",
    "           \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Writer agent error: {str(e)}\"\n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)],\n",
    "                \"next\": \"supervisor\",\n",
    "                \"current_agent\": \"writer\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": f\"Error generating report: {str(e)}\"\n",
    "            }\n",
    "   \n",
    "    return writer_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc861b",
   "metadata": {},
   "source": [
    "# Now bring in the Supervisor AI, which oversees and orchestrates the entire multi-agent workflow. This agent evaluates the current progress, knowing which team member just finished their task, and intelligently decides the next step: whether to continue with research, proceed to analysis, initiate report writing, or mark the project as complete. By parsing the conversation context and utilizing Gemini for reasoning, the supervisor agent ensures smooth transitions and quality control throughout the research pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f7f74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supervisor_agent(llm: ChatGoogleGenerativeAI, members: List[str]) -> callable:\n",
    "    \"\"\"Creates a supervisor agent to coordinate the team\"\"\"\n",
    "   \n",
    "    options = [\"FINISH\"] + members\n",
    "   \n",
    "    supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a Supervisor AI managing a research team. Your team members are:\n",
    "        {', '.join(members)}\n",
    "       \n",
    "        Your responsibilities:\n",
    "        1. Coordinate the workflow between team members\n",
    "        2. Ensure each agent completes their specialized tasks\n",
    "        3. Determine when the research is complete\n",
    "        4. Maintain quality standards throughout the process\n",
    "       \n",
    "        Given the conversation, determine the next step:\n",
    "        - If research is needed: route to \"researcher\"\n",
    "        - If analysis is needed: route to \"analyst\"  \n",
    "        - If report writing is needed: route to \"writer\"\n",
    "        - If work is complete: route to \"FINISH\"\n",
    "       \n",
    "        Available options: {options}\n",
    "       \n",
    "        Respond with just the name of the next agent or \"FINISH\".\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"Current status: {current_agent} just completed their task for topic: {research_topic}\")\n",
    "    ])\n",
    "   \n",
    "    supervisor_chain = supervisor_prompt | llm\n",
    "   \n",
    "    def supervisor_agent(state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute supervisor coordination\"\"\"\n",
    "        try:\n",
    "            response = supervisor_chain.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"current_agent\": state.get(\"current_agent\", \"none\"),\n",
    "                \"research_topic\": state[\"research_topic\"]\n",
    "            })\n",
    "           \n",
    "            next_agent = response.content.strip().lower()\n",
    "           \n",
    "            if \"finish\" in next_agent or \"complete\" in next_agent:\n",
    "                next_step = \"FINISH\"\n",
    "            elif \"research\" in next_agent:\n",
    "                next_step = \"researcher\"\n",
    "            elif \"analy\" in next_agent:\n",
    "                next_step = \"analyst\"\n",
    "            elif \"writ\" in next_agent:\n",
    "                next_step = \"writer\"\n",
    "            else:\n",
    "                current = state.get(\"current_agent\", \"\")\n",
    "                if current == \"researcher\":\n",
    "                    next_step = \"analyst\"\n",
    "                elif current == \"analyst\":\n",
    "                    next_step = \"writer\"\n",
    "                elif current == \"writer\":\n",
    "                    next_step = \"FINISH\"\n",
    "                else:\n",
    "                    next_step = \"researcher\"\n",
    "           \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=f\"Supervisor decision: Next agent is {next_step}\")],\n",
    "                \"next\": next_step,\n",
    "                \"current_agent\": \"supervisor\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "           \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Supervisor error: {str(e)}\"\n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)],\n",
    "                \"next\": \"FINISH\",\n",
    "                \"current_agent\": \"supervisor\",\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "   \n",
    "    return supervisor_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3075540",
   "metadata": {},
   "source": [
    "# Now assembling and executing the entire multi-agent workflow using LangGraph. First, we define the research team graph, which consists of nodes for each agent, Researcher, Analyst, Writer, and Supervisor, connected by logical transitions. Then, we compile this graph with memory using MemorySaver to persist conversation history. Finally, the run_research_team() function initializes the process with a topic and streams execution step by step, allowing us to track each agent’s contribution in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f2032c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_team_graph() -> StateGraph:\n",
    "    \"\"\"Creates the complete research team workflow graph\"\"\"\n",
    "   \n",
    "    llm = create_llm()\n",
    "   \n",
    "    members = [\"researcher\", \"analyst\", \"writer\"]\n",
    "    researcher = create_research_agent(llm)\n",
    "    analyst = create_analyst_agent(llm)\n",
    "    writer = create_writer_agent(llm)\n",
    "    supervisor = create_supervisor_agent(llm, members)\n",
    "   \n",
    "    workflow = StateGraph(AgentState)\n",
    "   \n",
    "    workflow.add_node(\"researcher\", researcher)\n",
    "    workflow.add_node(\"analyst\", analyst)\n",
    "    workflow.add_node(\"writer\", writer)\n",
    "    workflow.add_node(\"supervisor\", supervisor)\n",
    "   \n",
    "    workflow.add_edge(\"researcher\", \"supervisor\")\n",
    "    workflow.add_edge(\"analyst\", \"supervisor\")\n",
    "    workflow.add_edge(\"writer\", \"supervisor\")\n",
    "   \n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        lambda x: x[\"next\"],\n",
    "        {\n",
    "            \"researcher\": \"researcher\",\n",
    "            \"analyst\": \"analyst\",\n",
    "            \"writer\": \"writer\",\n",
    "            \"FINISH\": END\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "   \n",
    "    return workflow\n",
    "\n",
    "\n",
    "def compile_research_team():\n",
    "    \"\"\"Compile the research team graph with memory\"\"\"\n",
    "    workflow = create_research_team_graph()\n",
    "   \n",
    "    memory = MemorySaver()\n",
    "   \n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "   \n",
    "    return app\n",
    "\n",
    "\n",
    "def run_research_team(topic: str, thread_id: str = \"research_session_1\"):\n",
    "    \"\"\"Run the complete research team workflow\"\"\"\n",
    "   \n",
    "    app = compile_research_team()\n",
    "   \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=f\"Research the topic: {topic}\")],\n",
    "        \"research_topic\": topic,\n",
    "        \"next\": \"researcher\",\n",
    "        \"current_agent\": \"start\",\n",
    "        \"findings\": {},\n",
    "        \"final_report\": \"\"\n",
    "    }\n",
    "   \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "   \n",
    "    print(f\" Starting research on: {topic}\")\n",
    "    print(\"=\" * 50)\n",
    "   \n",
    "    try:\n",
    "        final_state = None\n",
    "        for step, state in enumerate(app.stream(initial_state, config=config)):\n",
    "            print(f\"\\n Step {step + 1}: {list(state.keys())[0]}\")\n",
    "           \n",
    "            current_state = list(state.values())[0]\n",
    "            if current_state[\"messages\"]:\n",
    "                last_message = current_state[\"messages\"][-1]\n",
    "                if isinstance(last_message, AIMessage):\n",
    "                    print(f\" {last_message.content[:200]}...\")\n",
    "           \n",
    "            final_state = current_state\n",
    "           \n",
    "            if step > 10:\n",
    "                print(\"  Maximum steps reached. Stopping execution.\")\n",
    "                break\n",
    "       \n",
    "        return final_state\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\" Error during execution: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fa1de",
   "metadata": {},
   "source": [
    "# Now our system with runtime and customization capabilities. The main block allows us to trigger a research run directly, making it perfect for testing the pipeline with a real-world topic, such as Artificial Intelligence in Healthcare. For more dynamic use, the interactive_research_session() enables multiple topic queries in a loop, simulating real-time exploration. Lastly, the create_custom_agent() function allows us to integrate new agents with unique roles and instructions, making the framework flexible and extensible for specialized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b782fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting research on: Artificial Intelligence in Education\n",
      "==================================================\n",
      "\n",
      " Step 1: supervisor\n",
      " Supervisor error: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"gen...\n",
      "\n",
      "==================================================\n",
      " FINAL RESULTS\n",
      "==================================================\n",
      " Final Agent: supervisor\n",
      " Findings: 0 sections\n",
      " Report Length: 0 characters\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = run_research_team(\"Artificial Intelligence in Education\")\n",
    "   \n",
    "    if result:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" FINAL RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\" Final Agent: {result['current_agent']}\")\n",
    "        print(f\" Findings: {len(result['findings'])} sections\")\n",
    "        print(f\" Report Length: {len(result['final_report'])} characters\")\n",
    "       \n",
    "        if result['final_report']:\n",
    "            print(\"\\n FINAL REPORT:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(result['final_report'])\n",
    "\n",
    "\n",
    "def interactive_research_session():\n",
    "    \"\"\"Run an interactive research session\"\"\"\n",
    "   \n",
    "    app = compile_research_team()\n",
    "   \n",
    "    print(\" Interactive Research Team Session\")\n",
    "    print(\"Enter 'quit' to exit\\n\")\n",
    "   \n",
    "    session_count = 0\n",
    "   \n",
    "    while True:\n",
    "        topic = input(\" Enter research topic: \").strip()\n",
    "       \n",
    "        if topic.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\" Goodbye!\")\n",
    "            break\n",
    "       \n",
    "        if not topic:\n",
    "            print(\" Please enter a valid topic.\")\n",
    "            continue\n",
    "       \n",
    "        session_count += 1\n",
    "        thread_id = f\"interactive_session_{session_count}\"\n",
    "       \n",
    "        result = run_research_team(topic, thread_id)\n",
    "       \n",
    "        if result and result['final_report']:\n",
    "            print(f\"\\n Research completed for: {topic}\")\n",
    "            print(f\" Report preview: {result['final_report'][:300]}...\")\n",
    "           \n",
    "            show_full = input(\"\\n Show full report? (y/n): \").lower()\n",
    "            if show_full.startswith('y'):\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\" COMPLETE RESEARCH REPORT\")\n",
    "                print(\"=\" * 60)\n",
    "                print(result['final_report'])\n",
    "       \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_custom_agent(role: str, instructions: str, llm: ChatGoogleGenerativeAI) -> callable:\n",
    "    \"\"\"Create a custom agent with specific role and instructions\"\"\"\n",
    "   \n",
    "    custom_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"You are a {role} AI.\n",
    "       \n",
    "        Your specific instructions:\n",
    "        {instructions}\n",
    "       \n",
    "        Always provide detailed, professional responses relevant to your role.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"Task: {task}\")\n",
    "    ])\n",
    "   \n",
    "    custom_chain = custom_prompt | llm\n",
    "   \n",
    "    def custom_agent(state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute custom agent task\"\"\"\n",
    "        try:\n",
    "            response = custom_chain.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                \"task\": state[\"research_topic\"]\n",
    "            })\n",
    "           \n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=response.content)],\n",
    "                \"next\": \"supervisor\",\n",
    "                \"current_agent\": role.lower().replace(\" \", \"_\"),\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "           \n",
    "        except Exception as e:\n",
    "            error_msg = f\"{role} agent error: {str(e)}\"\n",
    "            return {\n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=error_msg)],\n",
    "                \"next\": \"supervisor\",\n",
    "                \"current_agent\": role.lower().replace(\" \", \"_\"),\n",
    "                \"research_topic\": state[\"research_topic\"],\n",
    "                \"findings\": state.get(\"findings\", {}),\n",
    "                \"final_report\": state.get(\"final_report\", \"\")\n",
    "            }\n",
    "   \n",
    "    return custom_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0faf8",
   "metadata": {},
   "source": [
    "# We finalize the system by adding powerful utilities for graph visualization, performance monitoring, and a quick start demo. The visualize_graph() function provides a structural overview of agent connections, ideal for debugging or presentation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "149db3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph Research Team - Quick Start Demo\n",
      "==================================================\n",
      "\n",
      " Demo 1: Climate Change Impact on Human health\n",
      "----------------------------------------\n",
      " Starting research on: Climate Change Impact on Human health\n",
      "==================================================\n",
      "\n",
      " Step 1: supervisor\n",
      " Supervisor error: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"gen...\n",
      " Research failed\n",
      "\n",
      "==============================\n",
      "\n",
      " Demo 2: Quantum Computing Applications in Software Development\n",
      "----------------------------------------\n",
      " Starting research on: Quantum Computing Applications in Software Development\n",
      "==================================================\n",
      "\n",
      " Step 1: supervisor\n",
      " Supervisor error: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"gen...\n",
      " Research failed\n",
      "\n",
      "==============================\n",
      "\n",
      " Demo 3: Digital Privacy in the Modern Digital payment system\n",
      "----------------------------------------\n",
      " Starting research on: Digital Privacy in the Modern Digital payment system\n",
      "==================================================\n",
      "\n",
      " Step 1: supervisor\n",
      " Supervisor error: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"gen...\n",
      " Research failed\n",
      "\n",
      "==============================\n",
      " Demo completed!\n"
     ]
    }
   ],
   "source": [
    "def visualize_graph():\n",
    "    \"\"\"Visualize the research team graph structure\"\"\"\n",
    "   \n",
    "    try:\n",
    "        app = compile_research_team()\n",
    "       \n",
    "        graph_repr = app.get_graph()\n",
    "       \n",
    "        print(\"  Research Team Graph Structure\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Nodes: {list(graph_repr.nodes.keys())}\")\n",
    "        print(f\"Edges: {[(edge.source, edge.target) for edge in graph_repr.edges]}\")\n",
    "       \n",
    "        try:\n",
    "            graph_repr.draw_mermaid()\n",
    "        except:\n",
    "            print(\" Visual graph requires mermaid-py package\")\n",
    "            print(\"Install with: !pip install mermaid-py\")\n",
    "           \n",
    "    except Exception as e:\n",
    "        print(f\" Error visualizing graph: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def monitor_research_performance(topic: str):\n",
    "    \"\"\"Monitor and report performance metrics\"\"\"\n",
    "   \n",
    "    start_time = time.time()\n",
    "    print(f\"  Starting performance monitoring for: {topic}\")\n",
    "   \n",
    "    result = run_research_team(topic, f\"perf_test_{int(time.time())}\")\n",
    "   \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "   \n",
    "    metrics = {\n",
    "        \"duration\": duration,\n",
    "        \"total_messages\": len(result[\"messages\"]) if result else 0,\n",
    "        \"findings_sections\": len(result[\"findings\"]) if result else 0,\n",
    "        \"report_length\": len(result[\"final_report\"]) if result and result[\"final_report\"] else 0,\n",
    "        \"success\": result is not None\n",
    "    }\n",
    "   \n",
    "    print(\"\\n PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"  Duration: {duration:.2f} seconds\")\n",
    "    print(f\" Total Messages: {metrics['total_messages']}\")\n",
    "    print(f\" Findings Sections: {metrics['findings_sections']}\")\n",
    "    print(f\" Report Length: {metrics['report_length']} chars\")\n",
    "    print(f\" Success: {metrics['success']}\")\n",
    "   \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def quick_start_demo():\n",
    "    \"\"\"Complete demo of the research team system\"\"\"\n",
    "   \n",
    "    print(\" LangGraph Research Team - Quick Start Demo\")\n",
    "    print(\"=\" * 50)\n",
    "   \n",
    "    topics = [\n",
    "        \"Climate Change Impact on Human health\",\n",
    "        \"Quantum Computing Applications in Software Development\",\n",
    "        \"Digital Privacy in the Modern Digital payment system\"\n",
    "    ]\n",
    "   \n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        print(f\"\\n Demo {i}: {topic}\")\n",
    "        print(\"-\" * 40)\n",
    "       \n",
    "        try:\n",
    "            result = run_research_team(topic, f\"demo_{i}\")\n",
    "           \n",
    "            if result and result['final_report']:\n",
    "                print(f\" Research completed successfully!\")\n",
    "                print(f\" Report preview: {result['final_report'][:150]}...\")\n",
    "            else:\n",
    "                print(\" Research failed\")\n",
    "               \n",
    "        except Exception as e:\n",
    "            print(f\" Error in demo {i}: {str(e)}\")\n",
    "       \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "   \n",
    "    print(\" Demo completed!\")\n",
    "\n",
    "\n",
    "quick_start_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba3726",
   "metadata": {},
   "source": [
    "# In conclusion, we’ve successfully built and tested a fully functional, modular AI research assistant framework using LangGraph. With clear agent roles and automated task routing, we streamline research from raw topic input to a well-structured final report. Whether we use the quick start demo, run interactive sessions, or monitor performance, this system empowers us to handle complex research tasks with minimal intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
